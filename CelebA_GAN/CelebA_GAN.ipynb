{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os \nfrom PIL import Image\nIMAGES_PATH = '../input/img_align_celeba/img_align_celeba'\nimage_files = os.listdir('../input/img_align_celeba/img_align_celeba/')\nimage_files = [os.path.join(IMAGES_PATH, file_name) for file_name in image_files]\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"import numpy as np\nfrom PIL import Image\nidx = np.random.randint(len(image_files))\nimage = Image.open(image_files[idx])\nimage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Loader"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nimport torchvision.transforms as transforms\n\ndef load_image(img_path):\n    image = Image.open(img_path).convert('RGB')    \n    return image\nclass ImageDataSet(torch.utils.data.Dataset):\n\n    def __init__(self, root, image_loader=load_image, transform=None):\n        self.root = root\n        self.image_files = os.listdir(self.root)\n        self.loader = image_loader\n        self.transform = transform\n    def __len__(self):\n        # Here, we need to return the number of samples in this dataset.\n        return len(self.image_files)\n\n    def __getitem__(self, index):\n        images = self.loader(os.path.join(self.root, self.image_files[index]))\n        if self.transform is not None:\n            images = self.transform(images)\n        return images","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import DataLoader\n\ndef get_data_loader(image_dir = IMAGES_PATH, \n                    image_size= 64, batch_size=16, num_workers=0):\n\n    \n    # resize and normalize the images\n    transform = transforms.Compose([transforms.Resize((image_size, image_size)), # resize to 128x128\n                                    transforms.ToTensor()])\n\n\n    # define datasets using ImageFolder\n    dataset = ImageDataSet(image_dir, transform=transform)\n\n    # create and return DataLoaders\n    loader = DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n\n    return loader","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data_loader = get_data_loader()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision\nimport matplotlib.pyplot as plt\ndataiter = iter(data_loader)\nimages= dataiter.next()\n\n# show images\nfig = plt.figure(figsize=(12, 8))\nnpimg = torchvision.utils.make_grid(images).numpy()\nplt.imshow(np.transpose(npimg, (1, 2, 0)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndef scale(x, feature_range=(-1, 1)):\n    min_scale, max_scale = feature_range\n    x = x*(max_scale-min_scale) + min_scale\n    return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"img = images[0]\nscaled_img = scale(img)\n\nprint('Min: ', scaled_img.min())\nprint('Max: ', scaled_img.max())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\ndef convolution(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n\n    layers = []\n    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n    \n    layers.append(conv_layer)\n\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    return nn.Sequential(*layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\nimport torch.nn.functional as F\n\nclass Discriminator(nn.Module):\n\n    def __init__(self, conv_dim = 64):\n        super(Discriminator, self).__init__()\n        self.conv_dim = conv_dim\n        self.conv1 = convolution(3, conv_dim, 4, batch_norm=False) # x= 32, y = 32, depth 64\n        self.conv2 = convolution(conv_dim, conv_dim*2, 4) # (16, 16, 128)\n        self.conv3 = convolution(conv_dim*2, conv_dim*4, 4) # (8, 8, 256)\n        self.conv4 = convolution(conv_dim*4, conv_dim*8, 3, padding=1, stride =1) # (8, 8, 512)\n        \n        # Classification layer\n        self.fc = nn.Linear(conv_dim*8*8*8 , 1)\n    def forward(self, x):        \n        # relu applied to all conv layers but last\n        out = F.relu(self.conv1(x))\n        out = F.relu(self.conv2(out))\n        out = F.relu(self.conv3(out))\n        out = F.relu(self.conv4(out))\n        # last, classification layer\n        out = out.view(-1, self.conv_dim*8*8*8)\n        out = self.fc(out)\n        return out\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    \n    def __init__(self, z_size, conv_dim):\n        \"\"\"\n        Initialize the Generator Module\n        :param z_size: The length of the input latent vector, z\n        :param conv_dim: The depth of the inputs to the *last* transpose convolutional layer\n        \"\"\"\n        super(Generator, self).__init__()\n\n        # complete init function\n        \n\n    def forward(self, x):\n        \"\"\"\n        Forward propagation of the neural network\n        :param x: The input to the neural network     \n        :return: A 32x32x3 Tensor image as output\n        \"\"\"\n        # define feedforward behavior\n        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class ResidualBlock(nn.Module):\n    \"\"\"Defines a residual block.\n       This adds an input x to a convolutional layer (applied to x) with the same size input and output.\n       These blocks allow a model to learn an effective transformation from one domain to another.\n    \"\"\"\n    def __init__(self, conv_dim):\n        super(ResidualBlock, self).__init__()\n        \n        self.conv_layer1 = conv(in_channels=conv_dim, out_channels=conv_dim, \n                                kernel_size=3, stride=1, padding=1, batch_norm=True)\n        \n        self.conv_layer2 = conv(in_channels=conv_dim, out_channels=conv_dim, \n                               kernel_size=3, stride=1, padding=1, batch_norm=True)\n        \n    def forward(self, x):\n\n        out_1 = F.relu(self.conv_layer1(x))\n        out_2 = x + self.conv_layer2(out_1)\n        return out_2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def deconvolution(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\n    \"\"\"\n    layers = []\n    # append transpose conv layer\n    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n    # optional batch norm layer\n    if batch_norm:\n        layers.append(nn.BatchNorm2d(out_channels))\n    return nn.Sequential(*layers)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class Generator(nn.Module):\n    \n    def __init__(self, z_size, conv_dim=64):\n        super(Generator, self).__init__()\n        \n        self.conv_dim = conv_dim\n        \n        self.fc = nn.Linear(z_size, conv_dim*4*8*8)\n        self.deconv1 = deconvolution(conv_dim*4, conv_dim*2, 4)\n        self.deconv2 = deconvolution(conv_dim*2, conv_dim, 4)\n        self.deconv3 = deconvolution(conv_dim, 3, 4, batch_norm=False)\n        \n        \n\n    def forward(self, x):\n        x = self.fc(x)\n        x = x.view(-1, self.conv_dim*4, 4, 4)\n        x = F.relu(self.deconv1(x))\n        x = F.relu(self.deconv2(x))\n        x = F.tanh(self.deconv3(x))        \n        return x","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"conv_dim = 64\nz_size = 100\n\n# define discriminator and generator\nD = Discriminator(conv_dim)\nG = Generator(z_size=z_size, conv_dim=conv_dim)\n\nprint(D)\nprint()\nprint(G)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif train_on_gpu:\n    # move models to GPU\n    G.cuda()\n    D.cuda()\n    print('GPU available for training. Models moved to GPU')\nelse:\n    print('Training on CPU.')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def real_loss(D_out, smooth=False):\n    batch_size = D_out.size(0)\n    # label smoothing\n    if smooth:\n        # smooth, real labels = 0.9\n        labels = torch.ones(batch_size)*0.9\n    else:\n        labels = torch.ones(batch_size) # real labels = 1\n    # move labels to GPU if available     \n    if train_on_gpu:\n        labels = labels.cuda()\n    # binary cross entropy with logits loss\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    return loss\n\ndef fake_loss(D_out):\n    batch_size = D_out.size(0)\n    labels = torch.zeros(batch_size) # fake labels = 0\n    if train_on_gpu:\n        labels = labels.cuda()\n    criterion = nn.BCEWithLogitsLoss()\n    # calculate loss\n    loss = criterion(D_out.squeeze(), labels)\n    return loss","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.optim as optim\n\n# params\nlr = 0.0002\nbeta1=0.5\nbeta2=0.999 # default value\n\n# Create optimizers for the discriminator and generator\nd_optimizer = optim.Adam(D.parameters(), lr, [beta1, beta2])\ng_optimizer = optim.Adam(G.parameters(), lr, [beta1, beta2])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_epochs = 3\n\n# keep track of loss and generated, \"fake\" samples\nsamples = []\nlosses = []\n\nprint_every = 3000\n\n# Get some fixed data for sampling. These are images that are held\n# constant throughout training, and allow us to inspect the model's performance\nsample_size=16\nfixed_z = np.random.uniform(-1, 1, size=(sample_size, z_size))\nfixed_z = torch.from_numpy(fixed_z).float()\n\n# train the network\nfor epoch in range(num_epochs):\n    \n    for batch_i, real_images in enumerate(data_loader):\n                \n        batch_size = real_images.size(0)\n        \n        # important rescaling step\n        real_images = scale(real_images)\n        \n        # ============================================\n        #            TRAIN THE DISCRIMINATOR\n        # ============================================\n        \n        d_optimizer.zero_grad()\n        \n        # 1. Train with real images\n        \n        # Compute the discriminator losses on real images \n        if train_on_gpu:\n            real_images = real_images.cuda()\n        \n        D_real = D(real_images)\n        d_real_loss = real_loss(D_real)\n        \n        # 2. Train with fake images\n        \n        # Generate fake images\n        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n        z = torch.from_numpy(z).float()\n        # move x to GPU, if available\n        if train_on_gpu:\n            z = z.cuda()\n        fake_images = G(z)\n        \n        # Compute the discriminator losses on fake images            \n        D_fake = D(fake_images)\n        d_fake_loss = fake_loss(D_fake)\n        \n        # add up loss and perform backprop\n        d_loss = d_real_loss + d_fake_loss\n        d_loss.backward()\n        d_optimizer.step()\n        \n        \n        # =========================================\n        #            TRAIN THE GENERATOR\n        # =========================================\n        g_optimizer.zero_grad()\n        \n        # 1. Train with fake images and flipped labels\n        \n        # Generate fake images\n        z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n        z = torch.from_numpy(z).float()\n        if train_on_gpu:\n            z = z.cuda()\n        fake_images = G(z)\n        \n        # Compute the discriminator losses on fake images \n        # using flipped labels!\n        D_fake = D(fake_images)\n        g_loss = real_loss(D_fake) # use real loss to flip labels\n        \n        # perform backprop\n        g_loss.backward()\n        g_optimizer.step()\n\n        # Print some loss stats\n        if batch_i % print_every == 0:\n            # append discriminator loss and generator loss\n            losses.append((d_loss.item(), g_loss.item()))\n            # print discriminator and generator loss\n            print('Epoch [{:5d}/{:5d}] | d_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n                    epoch+1, num_epochs, d_loss.item(), g_loss.item()))\n\n    \n    ## AFTER EACH EPOCH##    \n    # generate and save sample, fake images\n    G.eval() # for generating samples\n    if train_on_gpu:\n        fixed_z = fixed_z.cuda()\n    samples_z = G(fixed_z)\n    samples.append(samples_z)\n    G.train() # back to training mode\n\n\n# Save training generator samples\nwith open('train_samples.pkl', 'wb') as f:\n    pkl.dump(samples, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def view_samples(epoch, samples):\n    fig, axes = plt.subplots(figsize=(16,4), nrows=2, ncols=8, sharey=True, sharex=True)\n    for ax, img in zip(axes.flatten(), samples[epoch]):\n        img = img.detach().cpu().numpy()\n        img = np.transpose(img, (1, 2, 0))\n        img = ((img + 1)*255 / (2)).astype(np.uint8)\n        ax.xaxis.set_visible(False)\n        ax.yaxis.set_visible(False)\n        im = ax.imshow(img.reshape((32,32,3)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with open('train_samples.pkl', 'rb') as f:\n    samples = pkl.load(f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"_ = view_samples(-1, samples)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}